{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d6344fce",
   "metadata": {},
   "source": [
    "# ESG Classification Task - Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51b3176d",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install --upgrade openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7a72ff1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccb57a47",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"your-openai-api-key\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe3c98d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_csv(\"dataset-path\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "240113a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "d_500 = dataset.groupby('mod_sub_topic').apply(lambda x: x.sample(n=500, random_state=42)).reset_index(drop=True)\n",
    "d_500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "866fd3ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_drop = ['domain', 'mod_topic', 'finbert_topic', 'finbert_sub_topic']\n",
    "d_500.drop(columns=columns_to_drop, axis=1, inplace=True)\n",
    "d_500.rename(columns={'text': 'prompt'}, inplace=True)\n",
    "d_500.rename(columns={'mod_sub_topic': 'completion'}, inplace=True)\n",
    "d_500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca23a797",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add a suffix separator to 'prompt' column\n",
    "d_500['prompt'] = d_500['prompt'] + ' ->'\n",
    "\n",
    "# Add a whitespace character to the beginning of 'completion' column\n",
    "d_500['completion'] = ' ' + d_500['completion']\n",
    "\n",
    "d_500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "234806ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "302af446",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = d_500['prompt']\n",
    "y = d_500['completion']\n",
    "\n",
    "# Splitting the data\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, stratify=y, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76d2ad2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert arrays to DataFrames\n",
    "d_x_train = pd.DataFrame(X_train, columns=['prompt'])\n",
    "d_y_train = pd.DataFrame(y_train, columns=['completion'])\n",
    "\n",
    "# Combine X_train and y_train into a single DataFrame\n",
    "d_500_train = pd.concat([d_x_train, d_y_train], axis=1)\n",
    "d_500_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a440e2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert arrays to DataFrames\n",
    "d_x_val = pd.DataFrame(X_val, columns=['prompt'])\n",
    "d_y_val = pd.DataFrame(y_val, columns=['completion'])\n",
    "\n",
    "# Combine X_train and y_train into a single DataFrame\n",
    "d_500_val = pd.concat([d_x_val, d_y_val], axis=1)\n",
    "d_500_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd7b511a",
   "metadata": {},
   "outputs": [],
   "source": [
    "d_500_train.to_json(\"d_500_train.jsonl\", orient=\"records\", lines=True)\n",
    "d_500_val.to_json(\"d_500_val.jsonl\", orient=\"records\", lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae7c0f84",
   "metadata": {},
   "outputs": [],
   "source": [
    "!openai api fine_tunes.create -t \"d_500_train.jsonl\" -v \"d_500_val.jsonl\" --compute_classification_metrics --classification_n_classes 9 -m ada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4aac5a37",
   "metadata": {},
   "outputs": [],
   "source": [
    "d_500_code_train = d_500_train.copy()\n",
    "d_500_code_val = d_500_val.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2777a8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "d_500_code_train.loc[d_500_code_train['completion'] == ' Climate Change', 'completion'] = ' baz'\n",
    "d_500_code_train.loc[d_500_code_train['completion'] == ' Resource Stewardship', 'completion'] = ' qux'\n",
    "d_500_code_train.loc[d_500_code_train['completion'] == ' Environmental Opportunities', 'completion'] = ' roc'\n",
    "d_500_code_train.loc[d_500_code_train['completion'] == ' Human Capital', 'completion'] = ' tuv'\n",
    "d_500_code_train.loc[d_500_code_train['completion'] == ' Product Liability', 'completion'] = ' dap'\n",
    "d_500_code_train.loc[d_500_code_train['completion'] == ' Social Opportunities', 'completion'] = ' stu'\n",
    "d_500_code_train.loc[d_500_code_train['completion'] == ' Corporate Governance', 'completion'] = ' klo'\n",
    "d_500_code_train.loc[d_500_code_train['completion'] == ' Business Ethics', 'completion'] = ' xya'\n",
    "d_500_code_train.loc[d_500_code_train['completion'] == ' Non-ESG', 'completion'] = ' nop'\n",
    "\n",
    "d_500_code_val.loc[d_500_code_val['completion'] == ' Climate Change', 'completion'] = ' baz'\n",
    "d_500_code_val.loc[d_500_code_val['completion'] == ' Resource Stewardship', 'completion'] = ' qux'\n",
    "d_500_code_val.loc[d_500_code_val['completion'] == ' Environmental Opportunities', 'completion'] = ' roc'\n",
    "d_500_code_val.loc[d_500_code_val['completion'] == ' Human Capital', 'completion'] = ' tuv'\n",
    "d_500_code_val.loc[d_500_code_val['completion'] == ' Product Liability', 'completion'] = ' dap'\n",
    "d_500_code_val.loc[d_500_code_val['completion'] == ' Social Opportunities', 'completion'] = ' stu'\n",
    "d_500_code_val.loc[d_500_code_val['completion'] == ' Corporate Governance', 'completion'] = ' klo'\n",
    "d_500_code_val.loc[d_500_code_val['completion'] == ' Business Ethics', 'completion'] = ' xya'\n",
    "d_500_code_val.loc[d_500_code_val['completion'] == ' Non-ESG', 'completion'] = ' nop'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a3529c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "d_500_code_train.to_json(\"d_500_code_train.jsonl\", orient=\"records\", lines=True)\n",
    "d_500_code_val.to_json(\"d_500_code_val.jsonl\", orient=\"records\", lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f287eb73",
   "metadata": {},
   "outputs": [],
   "source": [
    "!openai api fine_tunes.create -t \"d_500_code_train.jsonl\" -v \"d_500_code_val.jsonl\" --compute_classification_metrics --classification_n_classes 9 -m ada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dd705ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "d_500_prompt_train = d_500_train.copy()\n",
    "d_500_prompt_val = d_500_val.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b21d7e28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the classification classes and prompt\n",
    "sub_topics = [\" Climate Change\", \" Resource Stewardship\", \" Environmental Opportunities\", \" Human Capital\", \" Product Liability\", \" Social Opportunities\", \" Corporate Governance\", \" Business Ethics\", \" Non-ESG\"]\n",
    "prompt = \"Classify the following text into one of the following classes:\"\n",
    "\n",
    "# Function to concatenate the string to each value in the 'input_text' column\n",
    "def concatenate_prompt(row):\n",
    "    return f\"{prompt} {sub_topics} Text:\\n'''{row['prompt']}'''\"\n",
    "\n",
    "# Apply the function to create a new column 'prompt_text'\n",
    "d_500_prompt_train['prompt'] = d_500_prompt_train.apply(concatenate_prompt, axis=1)\n",
    "d_500_prompt_train['prompt'] = d_500_prompt_train['prompt'].str.replace(' ->', '')\n",
    "d_500_prompt_train['prompt'] = d_500_prompt_train['prompt'] + ' ->'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae0b0086",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the classification classes and prompt\n",
    "sub_topics = [\" Climate Change\", \" Resource Stewardship\", \" Environmental Opportunities\", \" Human Capital\", \" Product Liability\", \" Social Opportunities\", \" Corporate Governance\", \" Business Ethics\", \" Non-ESG\"]\n",
    "prompt = \"Classify the following text into one of the following classes:\"\n",
    "\n",
    "# Function to concatenate the string to each value in the 'input_text' column\n",
    "def concatenate_prompt(row):\n",
    "    return f\"{prompt} {sub_topics} Text:\\n'''{row['prompt']}'''\"\n",
    "\n",
    "# Apply the function to create a new column 'prompt_text'\n",
    "d_500_prompt_val['prompt'] = d_500_prompt_val.apply(concatenate_prompt, axis=1)\n",
    "d_500_prompt_val['prompt'] = d_500_prompt_val['prompt'].str.replace(' ->', '')\n",
    "d_500_prompt_val['prompt'] = d_500_prompt_val['prompt'] + ' ->'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52719e56",
   "metadata": {},
   "outputs": [],
   "source": [
    "d_500_prompt_train.to_json(\"d_500_prompt_train.jsonl\", orient=\"records\", lines=True)\n",
    "d_500_prompt_val.to_json(\"d_500_prompt_val.jsonl\", orient=\"records\", lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a164ee8",
   "metadata": {},
   "outputs": [],
   "source": [
    "!openai api fine_tunes.create -t \"d_500_prompt_train.jsonl\" -v \"d_500_prompt_val.jsonl\" --compute_classification_metrics --classification_n_classes 9 -m ada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1af0297a",
   "metadata": {},
   "outputs": [],
   "source": [
    "d_500_prompt_code_train = d_500_train.copy()\n",
    "d_500_prompt_code_val = d_500_val.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce7610a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the classification classes and prompt\n",
    "sub_topics = [\" baz\", \" qux\", \" roc\", \" tuv\", \" dap\", \" stu\", \" klo\", \" xya\", \" nop\"]\n",
    "prompt = \"Classify the following text into one of the following classes:\"\n",
    "\n",
    "# Function to concatenate the string to each value in the 'input_text' column\n",
    "def concatenate_prompt(row):\n",
    "    return f\"{prompt} {sub_topics} Text:\\n'''{row['prompt']}'''\"\n",
    "\n",
    "# Apply the function to create a new column 'prompt_text'\n",
    "d_500_prompt_code_train['prompt'] = d_500_prompt_code_train.apply(concatenate_prompt, axis=1)\n",
    "d_500_prompt_code_train['prompt'] = d_500_prompt_code_train['prompt'].str.replace(' ->', '')\n",
    "d_500_prompt_code_train['prompt'] = d_500_prompt_code_train['prompt'] + ' ->'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ab1e31c",
   "metadata": {},
   "outputs": [],
   "source": [
    "d_500_prompt_code_train.loc[d_500_prompt_code_train['completion'] == ' Climate Change', 'completion'] = ' baz'\n",
    "d_500_prompt_code_train.loc[d_500_prompt_code_train['completion'] == ' Resource Stewardship', 'completion'] = ' qux'\n",
    "d_500_prompt_code_train.loc[d_500_prompt_code_train['completion'] == ' Environmental Opportunities', 'completion'] = ' roc'\n",
    "d_500_prompt_code_train.loc[d_500_prompt_code_train['completion'] == ' Human Capital', 'completion'] = ' tuv'\n",
    "d_500_prompt_code_train.loc[d_500_prompt_code_train['completion'] == ' Product Liability', 'completion'] = ' dap'\n",
    "d_500_prompt_code_train.loc[d_500_prompt_code_train['completion'] == ' Social Opportunities', 'completion'] = ' stu'\n",
    "d_500_prompt_code_train.loc[d_500_prompt_code_train['completion'] == ' Corporate Governance', 'completion'] = ' klo'\n",
    "d_500_prompt_code_train.loc[d_500_prompt_code_train['completion'] == ' Business Ethics', 'completion'] = ' xya'\n",
    "d_500_prompt_code_train.loc[d_500_prompt_code_train['completion'] == ' Non-ESG', 'completion'] = ' nop'\n",
    "\n",
    "d_500_prompt_code_val.loc[d_500_prompt_code_val['completion'] == ' Climate Change', 'completion'] = ' baz'\n",
    "d_500_prompt_code_val.loc[d_500_prompt_code_val['completion'] == ' Resource Stewardship', 'completion'] = ' qux'\n",
    "d_500_prompt_code_val.loc[d_500_prompt_code_val['completion'] == ' Environmental Opportunities', 'completion'] = ' roc'\n",
    "d_500_prompt_code_val.loc[d_500_prompt_code_val['completion'] == ' Human Capital', 'completion'] = ' tuv'\n",
    "d_500_prompt_code_val.loc[d_500_prompt_code_val['completion'] == ' Product Liability', 'completion'] = ' dap'\n",
    "d_500_prompt_code_val.loc[d_500_prompt_code_val['completion'] == ' Social Opportunities', 'completion'] = ' stu'\n",
    "d_500_prompt_code_val.loc[d_500_prompt_code_val['completion'] == ' Corporate Governance', 'completion'] = ' klo'\n",
    "d_500_prompt_code_val.loc[d_500_prompt_code_val['completion'] == ' Corporate Behavior', 'completion'] = ' xya'\n",
    "d_500_prompt_code_val.loc[d_500_prompt_code_val['completion'] == ' Non-ESG', 'completion'] = ' nop'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a71c4008",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the classification classes and prompt\n",
    "sub_topics = [\" baz\", \" qux\", \" roc\", \" tuv\", \" dap\", \" stu\", \" klo\", \" xya\", \" nop\"]\n",
    "prompt = \"Classify the following text into one of the following classes:\"\n",
    "\n",
    "# Function to concatenate the string to each value in the 'input_text' column\n",
    "def concatenate_prompt(row):\n",
    "    return f\"{prompt} {sub_topics} Text:\\n'''{row['prompt']}'''\"\n",
    "\n",
    "# Apply the function to create a new column 'prompt_text'\n",
    "d_500_prompt_code_val['prompt'] = d_500_prompt_code_val.apply(concatenate_prompt, axis=1)\n",
    "d_500_prompt_code_val['prompt'] = d_500_prompt_code_val['prompt'].str.replace(' ->', '')\n",
    "d_500_prompt_code_val['prompt'] = d_500_prompt_code_val['prompt'] + ' ->'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a37d616",
   "metadata": {},
   "outputs": [],
   "source": [
    "d_500_prompt_code_train.to_json(\"d_500_prompt_code_train.jsonl\", orient=\"records\", lines=True)\n",
    "d_500_prompt_code_val.to_json(\"d_500_prompt_code_val.jsonl\", orient=\"records\", lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0691aa9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "!openai api fine_tunes.create -t \"d_500_prompt_code_train.jsonl\" -v \"d_500_prompt_code_val.jsonl\" --compute_classification_metrics --classification_n_classes 9 -m ada"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b16a8cc",
   "metadata": {},
   "source": [
    "# Zero-Shot Prompting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "252286a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "d_500_val['prompt'] = d_500_val['prompt'].str.replace(' ->', '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab5c3034",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_completion(prompt):\n",
    "    system_content = \"\"\"\n",
    "    You are an ESG news articles classifier. There are nine classes with these definitions:\\n\n",
    "    1. Climate Change: Pertains to news articles discussing topics such as carbon emissions reduction initiatives, the carbon footprint of products, climate change vulnerabilities, and financial initiatives or instruments designed to mitigate the impact of climate change. This can include policies or corporate strategies targeting climate change.\\n\n",
    "    2. Resource Stewardship: Involves articles highlighting how companies manage natural resources and waste. It encompasses a range of issues including but not limited to water conservation, biodiversity, sustainable land use, responsible raw material sourcing, toxic emissions reduction, and effective waste management, including electronic waste.\\n\n",
    "    3. Environmental Opportunities: Includes articles that focus on the potential opportunities arising from environmental conservation efforts. It covers green technology innovations, renewable energy initiatives, sustainable building projects, and financial investments targeting environmental sustainability.\\n\n",
    "    4. Human Capital: This class encompasses news items related to labor management, employee welfare, and workforce development. It can include articles about health & safety protocols, human capital development programs, and supply chain labor standards, including diversity, equity, and inclusion initiatives in the workplace.\\n\n",
    "    5. Product Liability: Relates to articles discussing aspects of product safety and quality, including chemical safety and consumer financial protection. It includes topics such as privacy and data security issues, and socially responsible investment, which might impact product liability.\\n\n",
    "    6. Social Opportunities: This category covers articles focusing on the societal benefits generated through corporate initiatives. It includes news on community financing, enhancing healthcare access, nutrition and health opportunities, educational initiatives, and investments aimed at social development.\\n\n",
    "    7. Corporate Governance: Deals with articles related to the structural and strategic management aspects of corporations. This includes topics such as ownership and control dynamics, board composition and performance, executive remuneration, accounting transparency, reorganization, and significant executive team changes, including the hiring or resignation of C-level executives and directors.\\n\n",
    "    8. Business Ethics: Encompasses articles on the ethical considerations of business operations. It includes subjects such as tax transparency, anti-corruption measures, fraud prevention, and adherence to ethical business practices and regulations.\\n\n",
    "    9. Non-ESG: This class is for articles that do not fit into any of the above ESG categories. It can include a wide range of topics not directly related to environmental, social, and governance issues.\\n\n",
    "    You need to classify the news articles into one of the classes\n",
    "    \"\"\"\n",
    "    \n",
    "    user_content = f\"\"\"\n",
    "    First, list CLUES (i.e., keywords, phrases, contextual information, semantic relations, semantic meaning, tones,\n",
    "    references) that support the class determination of input.\n",
    "    Second, deduce the diagnostic REASONING process from premises (i.e., clues, input) that supports the INPUT\n",
    "    class determination (limit the number of words to 130).\n",
    "    Third, based on clues, reasoning and input, determine the CLASS of INPUT.\n",
    "    \n",
    "    Construct the output into a json in this format:\n",
    "    {{\"clues\": \"\",\n",
    "      \"reasoning\": \"\",\n",
    "      \"class\": \"\"}}\n",
    "\n",
    "    \\nINPUT: `{prompt}`\n",
    "    \"\"\"\n",
    "\n",
    "    completion = openai.ChatCompletion.create(\n",
    "        model=\"gpt-3.5-turbo-0613\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system_content},\n",
    "            {\"role\": \"user\", \"content\": user_content}\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    completion_str = completion.choices[0].message\n",
    "\n",
    "    # Parse the \"content\" field to get the inner JSON\n",
    "    inner_completion = completion_str[\"content\"]\n",
    "    inner_json = json.loads(inner_completion)\n",
    "\n",
    "    # Extract the fields\n",
    "    clues = inner_json[\"clues\"]\n",
    "    reasoning = inner_json[\"reasoning\"]\n",
    "    _class = inner_json[\"class\"]\n",
    "\n",
    "    # Create a dictionary with the extracted data\n",
    "    data = {\n",
    "        \"clues\": [clues],\n",
    "        \"reasoning\": [reasoning],\n",
    "        \"class\": [_class]\n",
    "    }\n",
    "    return data\n",
    "\n",
    "# Load your prompts DataFrame\n",
    "prompts_df = d_500_val.copy()\n",
    "\n",
    "for index, row in prompts_df.iloc[:900].iterrows():\n",
    "    prompt = row['prompt']\n",
    "    output = generate_completion(prompt)\n",
    "    output[\"prompt\"] = prompt\n",
    "    # Save the dataframe\n",
    "    output_df = pd.DataFrame([output])\n",
    "    results_df_carp = pd.concat([results_df_carp, output_df], ignore_index=True)\n",
    "    # Check the current index\n",
    "    print(f\"Index: {index}\")\n",
    "    print(f\"Prompt: {prompt}\")\n",
    "    print(f\"Output: {output}\")\n",
    "\n",
    "# Save the results DataFrame to a CSV file\n",
    "results_df_carp.to_csv('results_df_carp.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94dddf40",
   "metadata": {},
   "source": [
    "# News Retrieval (NYT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09277bbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime, timedelta\n",
    "from dateutil.relativedelta import relativedelta\n",
    "\n",
    "# Replace 'your-api-key' with your actual API key\n",
    "API_KEY = 'your-api-key'\n",
    "\n",
    "def get_articles(year, month):\n",
    "    url = f'https://api.nytimes.com/svc/archive/v1/{year}/{month}.json?api-key={API_KEY}'\n",
    "    response = requests.get(url)\n",
    "    data = response.json()\n",
    "\n",
    "    articles = []\n",
    "    for article in data['response']['docs']:\n",
    "        keywords = [keyword['value'] for keyword in article['keywords'] if keyword['name'] in ['organizations', 'subject']]\n",
    "        articles.append({\n",
    "            'abstract': article.get('abstract'),\n",
    "            'web_url': article.get('web_url'),\n",
    "            'snippet': article.get('snippet'),\n",
    "            'lead_paragraph': article.get('lead_paragraph'),\n",
    "            'headline': article.get('headline', {}).get('main'),\n",
    "            'print_headline': article.get('headline', {}).get('print_headline'),\n",
    "            'keywords': keywords,\n",
    "            'pub_date': article.get('pub_date')\n",
    "        })\n",
    "    return articles\n",
    "\n",
    "start_date = datetime(2003, 1, 1)\n",
    "end_date = datetime(2022, 12, 31)\n",
    "\n",
    "date = start_date\n",
    "articles = {}\n",
    "current_year = start_date.year\n",
    "while date <= end_date:\n",
    "    print(f\"Getting articles for {date.year}-{date.month}\")\n",
    "\n",
    "    # Get articles\n",
    "    year_articles = get_articles(date.year, date.month)\n",
    "\n",
    "    # If the year has changed, save the previous year's articles to a DataFrame and CSV file\n",
    "    if date.year != current_year:\n",
    "        df = pd.DataFrame(articles[current_year])\n",
    "        df.replace('', np.nan, inplace=True)  # Replace empty strings with NaN\n",
    "        df.to_csv(f'/df_nyt_{current_year}.csv', index=False)\n",
    "        current_year = date.year\n",
    "\n",
    "    # Save the articles\n",
    "    if current_year in articles:\n",
    "        articles[current_year] += year_articles\n",
    "    else:\n",
    "        articles[current_year] = year_articles\n",
    "\n",
    "    # Move to next month\n",
    "    date += relativedelta(months=1)\n",
    "\n",
    "    # Wait 12 seconds to avoid rate limit\n",
    "    time.sleep(12)\n",
    "\n",
    "# Save the last year's articles to a DataFrame and CSV file\n",
    "df = pd.DataFrame(articles[current_year])\n",
    "df.replace('', np.nan, inplace=True)  # Replace empty strings with NaN\n",
    "df.to_csv(f'/df_nyt_{current_year}.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1d95589",
   "metadata": {},
   "source": [
    "# ESG Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b604ba47",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install spacy\n",
    "!python3 -m spacy download en_core_web_md"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1af39221",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load data\n",
    "df_nyt_2022 = pd.read_csv(\"NYT/df_nyt_2022.csv\")\n",
    "df_nyt_2021 = pd.read_csv(\"NYT/df_nyt_2021.csv\")\n",
    "df_nyt_2020 = pd.read_csv(\"NYT/df_nyt_2020.csv\")\n",
    "df_nyt_2019 = pd.read_csv(\"NYT/df_nyt_2019.csv\")\n",
    "df_nyt_2018 = pd.read_csv(\"NYT/df_nyt_2018.csv\")\n",
    "df_nyt_2017 = pd.read_csv(\"NYT/df_nyt_2017.csv\")\n",
    "df_nyt_2016 = pd.read_csv(\"NYT/df_nyt_2016.csv\")\n",
    "df_nyt_2015 = pd.read_csv(\"NYT/df_nyt_2015.csv\")\n",
    "df_nyt_2014 = pd.read_csv(\"NYT/df_nyt_2014.csv\")\n",
    "df_nyt_2013 = pd.read_csv(\"NYT/df_nyt_2013.csv\")\n",
    "df_nyt_2012 = pd.read_csv(\"NYT/df_nyt_2012.csv\")\n",
    "df_nyt_2011 = pd.read_csv(\"NYT/df_nyt_2011.csv\")\n",
    "df_nyt_2010 = pd.read_csv(\"NYT/df_nyt_2010.csv\")\n",
    "df_nyt_2009 = pd.read_csv(\"NYT/df_nyt_2009.csv\")\n",
    "df_nyt_2008 = pd.read_csv(\"NYT/df_nyt_2008.csv\")\n",
    "df_nyt_2007 = pd.read_csv(\"NYT/df_nyt_2007.csv\")\n",
    "df_nyt_2006 = pd.read_csv(\"NYT/df_nyt_2006.csv\")\n",
    "df_nyt_2005 = pd.read_csv(\"NYT/df_nyt_2005.csv\")\n",
    "df_nyt_2004 = pd.read_csv(\"NYT/df_nyt_2004.csv\")\n",
    "df_nyt_2003 = pd.read_csv(\"NYT/df_nyt_2003.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4f5cc45",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs = [\n",
    "    df_nyt_2022, \n",
    "    df_nyt_2021, \n",
    "    df_nyt_2020, \n",
    "    df_nyt_2019, \n",
    "    df_nyt_2018, \n",
    "    df_nyt_2017, \n",
    "    df_nyt_2016, \n",
    "    df_nyt_2015, \n",
    "    df_nyt_2014, \n",
    "    df_nyt_2013, \n",
    "    df_nyt_2012, \n",
    "    df_nyt_2011, \n",
    "    df_nyt_2010, \n",
    "    df_nyt_2009, \n",
    "    df_nyt_2008, \n",
    "    df_nyt_2007, \n",
    "    df_nyt_2006, \n",
    "    df_nyt_2005, \n",
    "    df_nyt_2004, \n",
    "    df_nyt_2003\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b7836ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "\n",
    "# Define the function to parse keywords\n",
    "def parse_keywords(keyword_str):\n",
    "    try:\n",
    "        return ast.literal_eval(keyword_str)\n",
    "    except (SyntaxError, ValueError):\n",
    "        return []\n",
    "\n",
    "# Loop over each company\n",
    "for index, row in df_companies.iterrows():\n",
    "    company_name = \"company-name\"\n",
    "    symbol = \"symbol\"\n",
    "\n",
    "    # List to store all the rows relevant to the current company across all years\n",
    "    all_rows = []\n",
    "\n",
    "    # Loop over each dataframe\n",
    "    for i, df in enumerate(dfs):\n",
    "        year = 2022 - i  # Calculate the year based on the index of the dataframe\n",
    "        \n",
    "        # Apply the function to parse keywords\n",
    "        df['parsed_keywords'] = df['keywords'].apply(parse_keywords)\n",
    "        \n",
    "        # Get all rows for the current company in the current dataframe and add them to the list\n",
    "        company_rows = df[df['parsed_keywords'].apply(lambda keywords: any(keyword for keyword in keywords if company_name in keyword))]\n",
    "        all_rows.append(company_rows)\n",
    "    \n",
    "    # Concatenate all the rows into a single dataframe for the current company\n",
    "    globals()[f\"{symbol.lower()}_df\"] = pd.concat(all_rows)\n",
    "\n",
    "# Now, you will have dataframes named like aapl_df, msft_df, etc., each containing data from 2022 to 2003."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "425e4dde",
   "metadata": {},
   "outputs": [],
   "source": [
    "company_df['text'] = company_df['headline'].str.cat(company_df['lead_paragraph'], sep='. ')\n",
    "company_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "725bfd14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill NaN values with an empty string\n",
    "company_df['text'].fillna(\"\", inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5db0820",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "# Load the pre-trained language model\n",
    "nlp = spacy.load('en_core_web_md')\n",
    "\n",
    "# Function to identify organizations in a text\n",
    "def identify_organizations(text):\n",
    "    doc = nlp(text)\n",
    "    orgs = [ent.text for ent in doc.ents if ent.label_ == \"ORG\"]\n",
    "    return orgs\n",
    "\n",
    "# Apply the function to every row in the 'text' column and store the results in a new column 'ner_result'\n",
    "company_df['ner_result'] = company_df['text'].apply(identify_organizations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38ca26bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "company_df = company_df[company_df['ner_result'].apply(lambda ner_result: isinstance(ner_result, list) and 'company_name' in ner_result)]\n",
    "company_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "833896e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, BertForSequenceClassification, pipeline\n",
    "\n",
    "finbert = BertForSequenceClassification.from_pretrained('yiyanghkust/finbert-esg-9-categories',num_labels=9)\n",
    "tokenizer = BertTokenizer.from_pretrained('yiyanghkust/finbert-esg-9-categories')\n",
    "esg_classifier = pipeline(\"text-classification\", model=finbert, tokenizer=tokenizer)\n",
    "\n",
    "results = esg_classifier('For 2002, our total net emissions were approximately 60 million metric tons of CO2 equivalents for all businesses and operations we have ﬁnancial interests in, based on its equity share in those businesses and operations.')\n",
    "print(results) # [{'label': 'Climate Change', 'score': 0.9955655932426453}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c815c3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "company_df['sub_label_text'] = None\n",
    "company_df['sub_score_text'] = None\n",
    "company_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d7fe153",
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, row in company_df.iterrows():\n",
    "    if isinstance(row['text'], str):\n",
    "        results_concat = nlp(row['text'])\n",
    "        company_df.at[index, 'sub_label_text'] = results_concat[0]['label']\n",
    "        company_df.at[index, 'sub_score_text'] = results_concat[0]['score']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4ecdbfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "company_df = company_df[company_df['sub_label_text'] != 'Non-ESG']\n",
    "company_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b4f4326",
   "metadata": {},
   "outputs": [],
   "source": [
    "company_df = company_df.reset_index(drop=True)\n",
    "company_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ca67e68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the classification classes and prompt\n",
    "sub_topics = [\"Climate Change\", \"Resource Stewardship\", \"Environmental Opportunities\", \"Human Capital\", \"Product Liability\", \"Social Opportunities\", \"Corporate Governance\", \"Business Ethics\", \"Non-ESG\"]\n",
    "prompt = \"Classify the following text into one of the following classes:\"\n",
    "\n",
    "# Function to concatenate the string to each value in the 'input_text' column\n",
    "def concatenate_prompt(row):\n",
    "    return f\"{prompt} {sub_topics} Text:\\n'''{row['text']}''' ->\"\n",
    "\n",
    "# Apply the function to create a new column 'prompt_text'\n",
    "company_df['prompt_text'] = company_df.apply(concatenate_prompt, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68506f49",
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, row in company_df.iloc[0:x].iterrows():\n",
    "    # Get the text from the current row\n",
    "    prompt = company_df.loc[index, 'prompt_text']\n",
    "    \n",
    "    # Get the completion from the OpenAI API\n",
    "    response = openai.Completion.create(\n",
    "        model=\"model-id\",\n",
    "        prompt=prompt,\n",
    "        temperature=0,\n",
    "        max_tokens=3,\n",
    "        top_p=1,\n",
    "        frequency_penalty=0,\n",
    "        presence_penalty=0\n",
    "    )\n",
    "    \n",
    "    print(response)\n",
    "    \n",
    "    # Get the text from the API response and store it in the DataFrame\n",
    "    company_df.loc[index, 'prompt_ada_result'] = response['choices'][0]['text'].strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01df4351",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"yiyanghkust/finbert-tone\")\n",
    "\n",
    "# Use a pipeline as a high-level helper\n",
    "from transformers import pipeline\n",
    "\n",
    "finbert_tone = pipeline(\"text-classification\", model=\"yiyanghkust/finbert-tone\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9081fbac",
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, row in msft_test.iterrows():\n",
    "    if isinstance(row['text'], str):\n",
    "        results_concat = finbert_tone(row['text'])\n",
    "        company_df.at[index, 'finbert_sentiment'] = results_concat[0]['label']\n",
    "        company_df.at[index, 'finbert_sentiment_score'] = results_concat[0]['score']"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
